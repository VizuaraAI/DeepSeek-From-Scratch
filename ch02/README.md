
# Chapter 2: The Road to MLA: Understanding the KV Cache Bottleneck

This chapter explores the foundational concepts of the attention mechanism and the inference bottleneck in autoregressive models. We examine the evolution of solutions, starting from the basic Key-Value Cache and progressing to more memory-efficient methods like Multi-Query Attention (MQA) and Grouped-Query Attention (GQA).

The code for this chapter, demonstrating these concepts, can be found in `01_main-chapter-code/Chapter_2.ipynb`.

### Relevant Videos for this Chapter

- **Attention Mechanism Overviews:**
  - [The Attention Mechanism 1 hour explanation](https://www.youtube.com/watch?v=K45ze9Yd5UE)
  - [Self Attention Mechanism - Handwritten from scratch](https://www.youtube.com/watch?v=s8mskq-nzec)
    [Causal Attention Explained: Don&#39;t Peek into the Future!](https://www.youtube.com/watch?v=c6Kkj6iLeBg "Ca")
- **Multi-Head Attention:**
  - [Multi-Head Attention Visually Explained](https://www.youtube.com/watch?v=qbN4ulK-bZA)
  - [Multi-Head Attention Handwritten from Scratch](https://www.youtube.com/watch?v=rvsEW-EsD-Y)
- **KV Cache and its Optimizations:**
  - [Key Value Cache from Scratch: The good side and the bad side](https://www.youtube.com/watch?v=IDwTiS4_bKo)
  - [Multi-Query Attention Explained | Dealing with KV Cache Memory Issues Part 1](https://www.youtube.com/watch?v=Z6B51Odtn-Y)
  - [Understand Grouped Query Attention (GQA) | The final frontier before latent attention](https://www.youtube.com/watch?v=kx3rETIxo4Q)
