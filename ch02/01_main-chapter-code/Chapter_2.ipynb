{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Solving the Inference Bottleneck with the Key-Value Cache\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The inference bottleneck is one of the most critical challenges in deploying large language models like DeepSeek. This chapter explores how the key-value cache, a fundamental optimization technique addresses this bottleneck and serves as the foundation for more advanced attention mechanisms.\n",
    "\n",
    "In autoregressive generation, each new token requires attention computations across all previous tokens in the sequence. Without optimizations, this would lead to:\n",
    "\n",
    "1. Quadratically increasing computation time as sequence length grows\n",
    "2. Redundant recomputation of key and value tensors for tokens that have already been processed\n",
    "3. Prohibitive memory and computational costs for practical applications\n",
    "\n",
    "The key-value cache solves these issues by storing previously computed key-value pairs, dramatically reducing the computational burden during token generation. This foundational technique enables DeepSeek's impressive performance with long contexts of up to 128K tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Understanding Attention Mechanisms\n",
    "\n",
    "Before diving into the key-value cache, we need to understand the attention mechanism itself. Attention allows a model to focus on relevant parts of the input when producing each element of the output.\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "The attention mechanism can be expressed mathematically as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ (queries): What we're looking for\n",
    "- $K$ (keys): What we match against\n",
    "- $V$ (values): What we extract when we find a match\n",
    "- $d_k$: The dimension of the keys (used for scaling)\n",
    "\n",
    "In transformer models like DeepSeek, each attention layer processes:\n",
    "- **Queries**: Representations of the token we're currently generating\n",
    "- **Keys/Values**: Representations of all tokens in the context\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "DeepSeek uses multi-head attention, which allows the model to attend to information from different representation subspaces simultaneously:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "Where each head is computed as:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "This creates multiple \"attention heads\" that can focus on different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Autoregressive Generation: The Root of the Inference Bottleneck\n",
    "\n",
    "DeepSeek models, like other transformer-based LLMs, generate text autoregressively—one token at a time, where each new token depends on all previous tokens. This creates a computational challenge during inference:\n",
    "\n",
    "1. For the first token, we compute attention using just the prompt\n",
    "2. For the second token, we compute attention using the prompt plus the first generated token\n",
    "3. For the third token, we compute attention using all previous tokens\n",
    "4. And so on...\n",
    "\n",
    "As the sequence grows, each new token requires more computation than the last. Without optimization, this would create:\n",
    "\n",
    "- **O(n²) complexity** in sequence length for each new token\n",
    "- **Redundant calculations** as the same keys and values are recomputed for existing tokens\n",
    "- **Slow inference speed** for practical applications\n",
    "\n",
    "The code below demonstrates this autoregressive generation process using a simple GPT-2 model. Pay attention to how we generate one token at a time, repeatedly passing the entire sequence through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "3c6fb8869aea4bde8babaf6f58196774",
      "b58398079b604abd9e539226583fb0f6",
      "11043264e399414b8aae337a70b1ade2",
      "b4e14a9aadcf41e58186443d860f9959",
      "a0dd354ac2fe4be8b09a592ccb246ee1",
      "ec0d91bc08f04e7581b8c203b59df151",
      "21f0d9970d7842ddb4599d4f6ac941bd",
      "442ee405777a45bfaa46b58190207b2c",
      "e3bc29fb23ee4ad3ac0dfab8cc783226",
      "38892e6fd82f4b9d891e415fd6ba173c",
      "28f92023f00049ca8edda62a690c1e40",
      "b72bd787868c4bc89299a7202daecb2d",
      "15333379b8d14983a8cdf9cfbf543055",
      "1376c1f607404b2cae08023dd8128186",
      "eb652fcf68e44a1ba34dcbf5e804bd4b",
      "8b61f43372294b13b8dbe3714fcee94a",
      "cfeadb80621841e5a0a620583f6364d0",
      "16f71b3b7383462f85f564a4c912bcbc",
      "747f75dcd1434bc9b6fa43e25b089bb1",
      "7e94ee19c5564293a0b79a3799b14f60",
      "095bdb833148444faae4c221f4ce7d92",
      "d3b951305dc94eaaaff9891debbe0a74",
      "a46dafddd2374cb89c9809ce83153ae6",
      "e6fa588c17234965ac0592adb92a8a22",
      "42483e0498da4014b87b877d866ff704",
      "648c14c74ca54de4bbf29c5f8fae9852",
      "33afbc2865a84f32b659b372effacae4",
      "893cf033fd4a4afd8d47bd7c6022b75e",
      "449a6125e6704e16ab2ae21e100179f2",
      "33cc0e29d59f45b49833986f0933a118",
      "9040d44ed8dd495f832a4b3b6731c0f8",
      "b0f8f2deb599451da5d22ad6f3a9d45e",
      "e10b1ea32ad244d4bb393e98b153ae7c",
      "291f6b542d63451a92394aa9b0e6fb55",
      "ebb98fe5b68b404abce5308c05c982c5",
      "f529a3e1b2544f3b90b0353ca7338b32",
      "a13023695fd54a86adcfd4db1efb6aa3",
      "2ca2332f2187412a8cd1a49f903c2e5d",
      "332fc3377462474ca929737bb526ca13",
      "052f0bd68134454b8a7b67c84df94eb6",
      "df65dc5f2cd54ee4a99e3bbe68f25cbc",
      "c5fde66b31cb481cb59bd808c2f473e1",
      "92d46f9649ac4ce18e75742866ec6195",
      "0dcbec3e19c44a189e9cfdd47f738bc7",
      "6cf44e85e07a4baab4f6a13bd3578eaa",
      "2106a2c9e4cc4f9ab3a24694cf761e55",
      "5545123e4d0d4422a2a24b9da00c6978",
      "3bfa46f31a9440c3984b74ef1343e017",
      "96fd102f65654437abc05764c64882f9",
      "c751c0f4f58d436199121e04a3867aa4",
      "879ab52607b64c958fea51060f39a449",
      "56e2d73e360148eeb75b992eeffe88b3",
      "9bdd7493f3ef4a0a86989b247b316ab2",
      "d12ac667e31a4239b8cb5607a3858d0a",
      "83bf5de1855b4b8ea680c124d00a7322",
      "a3e4d97a483f42cdab58a895f3d4592a",
      "7caafb6233b34205af6d628a6d8b4f52",
      "08a87674c4754e76996fcda567d0d24b",
      "59687720f4104305be57e95ba473c8b3",
      "0957b4248436451f8e06914726af1dd1",
      "ddb1dcde991d4f1080c941b9a53c2291",
      "da0e9ee9197144eda786ed90387c7293",
      "b8ba4a13626e49bf8cb56bba2fe41745",
      "23262e267259436eb6200ecab49cd30d",
      "3a5b445740fc4a969238fa45246a3be1",
      "8f84a70ca6494fafb32e2b0818020a2e",
      "fa966abcb988433e88bb82b22051eb2d",
      "21a459ec42a64e78b898846da5361453",
      "e1350e3d3263478dab315608ac003144",
      "3fe9b1f9f5eb4bcd84a6a34bfefa6828",
      "a8bde1135b4748059c7352d3a0830151",
      "32e25e6f39b74a77bf7d41308705344b",
      "42465261d2964accac301b16dbb834aa",
      "ea51759bcd3d474993c7122a28765476",
      "3f6257a316094d9d95a11bb0bdfdf58a",
      "a213bbb40b6c420999320afddd1f612d",
      "755621462ee443fcb684c879fe68855e"
     ]
    },
    "id": "v5Lf7DmHBz0g",
    "outputId": "a6f7a270-e680-4a8b-c26c-56775412de51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6fb8869aea4bde8babaf6f58196774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72bd787868c4bc89299a7202daecb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46dafddd2374cb89c9809ce83153ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291f6b542d63451a92394aa9b0e6fb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf44e85e07a4baab4f6a13bd3578eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e4d97a483f42cdab58a895f3d4592a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa966abcb988433e88bb82b22051eb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n",
      "Prompt: 'The next day is bright' and sunny, and the sun is shining. The sun is shining, and the moon is shining.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# SETUP: Imports and Model Loading for the entire notebook\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "print(\"Setting up models...\")\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "#\n",
    "# LISTING 2.1: Visualizing autoregressive generation with GPT-2\n",
    "#\n",
    "prompt = \"The next day is bright\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\", end=\"\")\n",
    "\n",
    "# Generate 20 tokens\n",
    "for _ in range(20):\n",
    "    # Pass the entire sequence to the model\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the logits for the very last token\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "\n",
    "    # Get the ID of the most likely next token (greedy decoding)\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Append the new token ID to the input sequence\n",
    "    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "    # Decode and print the new token\n",
    "    new_token = tokenizer.decode(next_token_id[0])\n",
    "    print(new_token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 2.1: Visualizing Autoregressive Generation\n",
    "\n",
    "The code below demonstrates the token-by-token generation process in an autoregressive model. We start with a prompt and then generate 20 additional tokens sequentially. Notice how:\n",
    "\n",
    "1. The entire sequence is passed through the model at each step\n",
    "2. We extract only the logits for the last token position\n",
    "3. We select the most likely next token (greedy decoding)\n",
    "4. We append this token to our sequence and repeat\n",
    "\n",
    "This naive implementation clearly shows why autoregressive generation becomes increasingly slow as the sequence grows longer. Each token requires a full forward pass through all previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thQ864e8elIX",
    "outputId": "b08919d0-203a-4fe1-d5f1-a101f8d8cf76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating without KV Cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time without KV Cache: 57.6215 seconds\n",
      "\n",
      "Generating with KV Cache...\n",
      "Time with KV Cache: 6.3934 seconds\n",
      "\n",
      "KV Cache Speedup: 9.01x\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# LISTING 2.2: Demonstrating the Speedup of KV Caching\n",
    "#\n",
    "prompt = \"The next day is bright\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# --- Timing without KV cache ---\n",
    "print(\"Generating without KV Cache...\")\n",
    "start_time_without_cache = time.time()\n",
    "output_without_cache = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=False, # Explicitly disable the cache\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "end_time_without_cache = time.time()\n",
    "duration_without_cache = end_time_without_cache - start_time_without_cache\n",
    "print(f\"Time without KV Cache: {duration_without_cache:.4f} seconds\\n\")\n",
    "\n",
    "\n",
    "# --- Timing with KV cache ---\n",
    "print(\"Generating with KV Cache...\")\n",
    "start_time_with_cache = time.time()\n",
    "output_with_cache = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True, # Explicitly enable the cache\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "end_time_with_cache = time.time()\n",
    "duration_with_cache = end_time_with_cache - start_time_with_cache\n",
    "print(f\"Time with KV Cache: {duration_with_cache:.4f} seconds\\n\")\n",
    "\n",
    "\n",
    "# --- Calculate and print the speedup ---\n",
    "speedup = duration_without_cache / duration_with_cache\n",
    "print(f\"KV Cache Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Key-Value Caching: The First Generation Solution\n",
    "\n",
    "The key-value (KV) cache is the foundational technique for addressing the inference bottleneck in transformer models like DeepSeek. The key insight is simple but powerful:\n",
    "\n",
    "**Since previous tokens don't change during generation, their key and value projections can be computed once and reused.**\n",
    "\n",
    "### How KV Caching Works:\n",
    "\n",
    "1. **Initial Computation**: For the first token, compute Q, K, V as usual\n",
    "2. **Storage**: Save the K, V tensors in a \"cache\" \n",
    "3. **Subsequent Tokens**: \n",
    "   - Only compute Q, K, V for the new token\n",
    "   - Retrieve previous K, V from the cache\n",
    "   - Concatenate the new K, V with the cached ones\n",
    "   - Store the expanded K, V in the cache\n",
    "4. **Result**: Each new token only needs to compute its own key-value pair rather than recomputing for all tokens\n",
    "\n",
    "This reduces the computational complexity from O(n²) to O(n) per token, where n is the sequence length.\n",
    "\n",
    "### Memory-Computation Tradeoff:\n",
    "\n",
    "The KV cache trades increased memory usage for significantly reduced computation:\n",
    "\n",
    "- **Memory Usage**: Increases linearly with sequence length\n",
    "- **Computation**: Dramatically reduced for long sequences\n",
    "\n",
    "This is particularly important for DeepSeek models with their 128K context window capability.\n",
    "\n",
    "### Listing 2.2: Benchmarking KV Cache Performance\n",
    "\n",
    "The code below demonstrates and measures the dramatic speedup achieved through KV caching:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 From KV Cache to Advanced Attention Mechanisms\n",
    "\n",
    "While the key-value cache dramatically improves inference speed, models like DeepSeek require further optimizations to handle their massive scale efficiently. The KV cache serves as the foundation for more advanced attention mechanisms:\n",
    "\n",
    "### The Evolution Path:\n",
    "\n",
    "1. **KV Cache (First Generation)**: The basic optimization we've just explored\n",
    "2. **Multi-Query Attention (MQA)**: Shares K/V projections across attention heads\n",
    "3. **Grouped-Query Attention (GQA)**: A middle ground between standard attention and MQA\n",
    "4. **Multi-Head Latent Attention**: Advanced techniques used in the latest DeepSeek models\n",
    "\n",
    "Let's explore these evolved attention mechanisms that build on the KV cache foundation.\n",
    "\n",
    "### Multi-Query Attention (MQA)\n",
    "\n",
    "Standard multi-head attention requires unique K, V projections for each attention head, which increases the KV cache size proportionally to the number of heads. Multi-Query Attention addresses this by:\n",
    "\n",
    "- Using a **single shared K, V projection** across all attention heads\n",
    "- Maintaining **separate Q projections** for each head\n",
    "\n",
    "This significantly reduces memory requirements during inference while maintaining most of the model's capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped-Query Attention (GQA)\n",
    "\n",
    "While MQA significantly reduces memory usage, it can sometimes degrade model quality. DeepSeek models use Grouped-Query Attention (GQA) as a balanced approach:\n",
    "\n",
    "- Group attention heads into clusters (e.g., 4-8 groups for 32 heads)\n",
    "- Each group shares the same K,V projections\n",
    "- Queries remain separate for each head\n",
    "\n",
    "GQA offers a favorable trade-off:\n",
    "- Better quality than MQA (more expressive)\n",
    "- More efficient than standard multi-head attention\n",
    "- Well-suited for DeepSeek's massive scale\n",
    "\n",
    "This approach is particularly important for DeepSeek's 671B parameter models where balancing efficiency and quality is crucial.\n",
    "\n",
    "### Comparison of Attention Variants\n",
    "\n",
    "| Feature | Standard Multi-Head | Grouped-Query | Multi-Query |\n",
    "|---------|---------------------|---------------|-------------|\n",
    "| K,V Projections | One per head | One per group | One shared |\n",
    "| Cache Size | Largest | Medium | Smallest |\n",
    "| Quality | Highest | Good | Lower |\n",
    "| Memory Efficiency | Low | Medium | High |\n",
    "\n",
    "DeepSeek models use these optimizations strategically depending on the model size and intended use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5VFUZtefJwg",
    "outputId": "a901efc5-b871-48f9-c2da-fdbe191e070f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MQA Layer successful!\n",
      "Input shape: torch.Size([4, 64, 512])\n",
      "Output shape: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# LISTING 2.3: Implementing an MQA layer from scratch\n",
    "#\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, self.d_head) # Single projection for K\n",
    "        self.W_v = nn.Linear(d_model, self.d_head) # Single projection for V\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Using a fixed size mask for demonstration. A dynamic one is better in practice.\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(1, 1, 1024, 1024), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Query: (B, num_heads, seq_len, d_head)\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Key & Value: (B, 1, seq_len, d_head)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, 1, self.d_head).transpose(1, 2)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, 1, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Repeat K and V for each query head\n",
    "        k = k.repeat(1, self.num_heads, 1, 1) # (B, num_heads, seq_len, d_head)\n",
    "        v = v.repeat(1, self.num_heads, 1, 1) # (B, num_heads, seq_len, d_head)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        # Apply causal mask\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:,:,:seq_len,:seq_len] == 1, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = (attn_weights @ v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.W_o(context_vector)\n",
    "        return output\n",
    "\n",
    "# --- Usage Example ---\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "mqa_layer = MultiQueryAttention(d_model, num_heads)\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "output = mqa_layer(dummy_input)\n",
    "\n",
    "print(\"MQA Layer successful!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 2.3: Implementing Multi-Query Attention (MQA)\n",
    "\n",
    "The code below implements a Multi-Query Attention layer from scratch. Notice the key differences from standard multi-head attention:\n",
    "\n",
    "1. Query projection (`self.W_q`) maps to the full model dimension (`d_model`)\n",
    "2. Key and value projections (`self.W_k` and `self.W_v`) map to just a single head dimension (`self.d_head`)\n",
    "3. We use `repeat()` to duplicate the single key and value for all query heads\n",
    "\n",
    "This implementation clearly shows how MQA reduces the parameter count and memory footprint during inference, especially for the KV cache which only needs to store a single key-value pair per token instead of one per attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VWTyLK4hhDy",
    "outputId": "a2090554-a45a-4c0c-ef73-08c7a01b3ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GQA Layer successful!\n",
      "Input shape: torch.Size([4, 64, 512])\n",
      "Output shape: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# LISTING 2.4: Implementing a GQA layer from scratch\n",
    "#\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_groups, dropout=0.0, max_seq_len: int = 1024):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        assert num_heads % num_groups == 0, \"num_heads must be divisible by num_groups\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, self.num_groups * self.d_head) # Grouped projection for K\n",
    "        self.W_v = nn.Linear(d_model, self.num_groups * self.d_head) # Grouped projection for V\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._register_mask_buffer(max_seq_len)\n",
    "\n",
    "    def _register_mask_buffer(self, max_seq_len):\n",
    "        if max_seq_len > 0:\n",
    "            mask = torch.triu(torch.ones(1, 1, max_seq_len, max_seq_len, dtype=torch.bool), diagonal=1)\n",
    "            self.register_buffer(\"causal_mask\", mask, persistent=False)\n",
    "        else:\n",
    "            self.causal_mask = None\n",
    "\n",
    "    def _get_causal_mask(self, seq_len, device):\n",
    "        if self.causal_mask is not None and self.causal_mask.size(-1) >= seq_len:\n",
    "            return self.causal_mask[:, :, :seq_len, :seq_len]\n",
    "        # Dynamically create mask if needed\n",
    "        return torch.triu(torch.ones(1, 1, seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Query: (B, num_heads, T, d_head)\n",
    "        q = self.W_q(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Key & Value: (B, num_groups, T, d_head)\n",
    "        k = self.W_k(x).view(B, T, self.num_groups, self.d_head).transpose(1, 2)\n",
    "        v = self.W_v(x).view(B, T, self.num_groups, self.d_head).transpose(1, 2)\n",
    "\n",
    "        heads_per_group = self.num_heads // self.num_groups\n",
    "\n",
    "        # Repeat K and V to match query heads\n",
    "        k = k.repeat_interleave(heads_per_group, dim=1) # (B, num_heads, T, d_head)\n",
    "        v = v.repeat_interleave(heads_per_group, dim=1) # (B, num_heads, T, d_head)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * (self.d_head**-0.5)\n",
    "\n",
    "        causal_mask = self._get_causal_mask(T, x.device)\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context = (attn_weights @ v).transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "\n",
    "        return self.W_o(context)\n",
    "\n",
    "# --- Usage Example ---\n",
    "d_model = 512\n",
    "num_heads = 32\n",
    "num_groups = 4 # e.g., Llama 2 7B uses 4 groups for 32 heads\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "gqa_layer = GroupedQueryAttention(d_model, num_heads, num_groups)\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "output = gqa_layer(dummy_input)\n",
    "\n",
    "print(\"GQA Layer successful!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 2.4: Implementing Grouped-Query Attention (GQA)\n",
    "\n",
    "The code below implements a Grouped-Query Attention layer from scratch. Note the key differences:\n",
    "\n",
    "1. Query projection (`self.W_q`) still maps to the full model dimension\n",
    "2. Key and value projections (`self.W_k` and `self.W_v`) map to `self.num_groups * self.d_head`\n",
    "3. We use `repeat_interleave()` to match each key and value group with its corresponding query heads\n",
    "\n",
    "This implementation demonstrates how GQA balances between standard multi-head attention and MQA:\n",
    "- Fewer K,V projections than standard attention (reduced by factor of `num_heads/num_groups`)\n",
    "- More K,V diversity than MQA (one set per group rather than just one shared)\n",
    "\n",
    "DeepSeek models use this approach to maintain quality while reducing memory requirements, especially for the KV cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Conclusion: The Key-Value Cache as Foundation\n",
    "\n",
    "The key-value cache represents the first major breakthrough in addressing the inference bottleneck for transformer models, and it serves as the foundation for all subsequent attention optimizations in DeepSeek models:\n",
    "\n",
    "1. **Fundamental Optimization**: By storing and reusing key-value pairs, the KV cache dramatically reduces the computational cost of autoregressive generation.\n",
    "\n",
    "2. **Memory-Computation Tradeoff**: The KV cache exemplifies an essential engineering principle—trading increased memory usage for reduced computation, which is often beneficial for practical applications.\n",
    "\n",
    "3. **Foundation for Advanced Techniques**: The MQA and GQA techniques build directly on the KV cache foundation, further optimizing memory usage while maintaining model quality.\n",
    "\n",
    "4. **Enabling Long Context**: DeepSeek's impressive 128K token context window capability would be impossible without these attention optimizations.\n",
    "\n",
    "Understanding the key-value cache and its evolved forms is essential for grasping how DeepSeek models achieve their remarkable balance of quality and efficiency at scale. As we'll see in the next chapter, these attention optimizations work in concert with DeepSeek's Mixture of Experts (MoE) architecture to enable its massive 671B parameter scale while keeping activated parameters at just 37B."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
