
# Chapter 3: The DeepSeek Breakthrough: Multi-Head Latent Attention (MLA)

Dive into the core innovation of DeepSeek: Multi-Head Latent Attention (MLA). This chapter breaks down the "compress for storage, decompress for use" paradigm that dramatically reduces the KV cache memory footprint. We also explore how MLA is fused with a decoupled Rotary Positional Encoding (RoPE) to create a complete, state-of-the-art attention mechanism.

The code for this chapter, including from-scratch implementations of MLA and the fused MLA-RoPE module, can be found in `01_main-chapter-code/Chapter_3.ipynb`.

### Relevant Videos for this Chapter

- **Multi-Head Latent Attention (MLA):**
  - [Multi-Head Latent Attention From Scratch | One of the major DeepSeek innovation](https://www.youtube.com/watch?v=NlDQUj1olXM)
  - [Multi-Head Latent Attention Coded from Scratch in Python](https://www.youtube.com/watch?v=mIaWmJVrMpc)
- **Journey to Rotary Positional Encodings (RoPE):**
  - [Integer and Binary Positional Encodings](https://www.youtube.com/watch?v=rP0CoTxe5gU)
  - [All about Sinusoidal Positional Encodings](https://www.youtube.com/watch?v=bQCQ7VO-TWU)
  - [Rotary Positional Encodings | Explained Visually](https://www.youtube.com/watch?v=a17DlNxkv2k)
- **Putting It All Together:**
  - [How DeepSeek exactly implemented Latent Attention | MLA + RoPE](https://www.youtube.com/watch?v=m1x8vA_Tscc)
