{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c741b5",
   "metadata": {},
   "source": [
    "# Chapter 4: Mixture of Experts in DeepSeek\n",
    "\n",
    "Large Language Models have grown exponentially in size, with parameters reaching into the hundreds of billions. This growth presents significant computational challenges, particularly for inference. DeepSeek models use a **Mixture of Experts (MoE)** architecture to tackle this problem.\n",
    "\n",
    "## What is Mixture of Experts?\n",
    "\n",
    "MoE is a neural network architecture that employs multiple specialized sub-networks (experts) and a routing mechanism to direct different inputs to the appropriate experts. This approach allows models to scale to enormous parameter counts while keeping computational costs manageable.\n",
    "\n",
    "In this chapter, we'll implement the DeepSeek MoE architecture, which includes:\n",
    "\n",
    "1. **Shared Experts** - Always active for all tokens\n",
    "2. **Routed Experts** - Selectively activated based on token content\n",
    "3. **Load Balancing** - Techniques to ensure even expert utilization\n",
    "\n",
    "The DeepSeek approach combines the efficiency of sparse activation with the reliability of always-on shared experts, creating a robust and computationally efficient architecture.\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "Let's begin by importing the necessary Python libraries:\n",
    "\n",
    "- `math`: For mathematical operations like square root and pi\n",
    "- `contextlib.nullcontext`: For optional context management\n",
    "- `typing.Optional`: For type hints with optional parameters\n",
    "- `torch`: PyTorch library for tensor operations and neural networks\n",
    "- `torch.nn`: Neural network modules and functions\n",
    "- `torch.nn.functional`: Functional interface for neural network operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from contextlib import nullcontext \n",
    "from typing import Optional \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303dc67d",
   "metadata": {},
   "source": [
    "## Core MoE Components\n",
    "\n",
    "In this notebook, we'll implement two critical components of the DeepSeek MoE architecture:\n",
    "\n",
    "1. **ExpertFFN**: The individual expert networks\n",
    "2. **DeepSeekMoE**: The complete MoE layer with routing\n",
    "\n",
    "Let's examine how DeepSeek designs these components to achieve an efficient balance between parameter count and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Slightly faster GELU (approx)\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) *\n",
    "                                       (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class ExpertFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer MLP expert. Hidden dim is usually smaller than a dense FFN\n",
    "    (e.g., 0.25 × d_model in DeepSeek-V3).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, hidden: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.dropout(_gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSeek-V3 style Mixture-of-Experts (MoE) layer.\n",
    "\n",
    "    This MoE layer incorporates both routed experts (selected by a router)\n",
    "    and shared experts (applied to all inputs). It is designed based on\n",
    "    the architecture described in the DeepSeek-V3 paper.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output features.\n",
    "        n_routed_exp (int): The number of routed experts.\n",
    "        n_shared_exp (int, optional): The number of shared experts. Defaults to 1.\n",
    "        top_k (int, optional): The number of routed experts to select for each token.\n",
    "                               Defaults to 8.\n",
    "        routed_hidden (int, optional): The hidden dimension for routed experts.\n",
    "                                      Defaults to 2048.\n",
    "        shared_hidden (Optional[int], optional): The hidden dimension for shared experts.\n",
    "                                                If None, uses routed_hidden. Defaults to None.\n",
    "        bias_lr (float, optional): Learning rate for the router bias (updated online).\n",
    "                                   Defaults to 0.01.\n",
    "        fp16_router (bool, optional): Whether to use FP16 precision for router calculations.\n",
    "                                     Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_routed_exp: int,\n",
    "        n_shared_exp: int = 1,\n",
    "        top_k: int = 8,\n",
    "        routed_hidden: int = 2_048,\n",
    "        shared_hidden: Optional[int] = None,\n",
    "        bias_lr: float = 0.01,\n",
    "        fp16_router: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Assert that the number of selected experts (top_k) is less than or equal to the total number of routed experts.\n",
    "        assert top_k <= n_routed_exp, \"k must be ≤ number of routed experts\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_routed = n_routed_exp\n",
    "        self.n_shared = n_shared_exp\n",
    "        self.top_k = top_k\n",
    "        self.bias_lr = bias_lr\n",
    "        self.fp16_router = fp16_router\n",
    "\n",
    "        # Module list for the routed experts.\n",
    "        self.routed = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, routed_hidden) for _ in range(n_routed_exp)]\n",
    "        )\n",
    "        # Determine the hidden dimension for shared experts. Use routed_hidden if shared_hidden is not provided.\n",
    "        hidden_shared = shared_hidden or routed_hidden\n",
    "        # Module list for the shared experts.\n",
    "        self.shared = nn.ModuleList(\n",
    "            [ExpertFFN(d_model, hidden_shared) for _ in range(n_shared_exp)]\n",
    "        )\n",
    "\n",
    "        # Register a parameter for the centroids used by the router.\n",
    "        # Centroids represent the \"preference\" of each expert for different input features.\n",
    "        self.register_parameter(\"centroids\", nn.Parameter(torch.empty(n_routed_exp, d_model)))\n",
    "        # Initialize centroids with a normal distribution.\n",
    "        nn.init.normal_(self.centroids, std=d_model ** -0.5)\n",
    "\n",
    "        # Register a buffer for the router bias. This bias is updated online\n",
    "        # without using standard gradient descent, hence it's not a parameter.\n",
    "        self.register_buffer(\"bias\", torch.zeros(n_routed_exp))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DeepSeekMoE layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], where B is\n",
    "                              batch size, S is sequence length, and D is d_model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape [B, S, D], which is the\n",
    "                          sum of the input, shared expert outputs, and routed\n",
    "                          expert outputs.\n",
    "        \"\"\"\n",
    "        # Get dimensions of the input tensor.\n",
    "        B, S, D = x.shape\n",
    "        # Reshape the input to [N, D], where N = B * S (number of tokens).\n",
    "        x_flat = x.reshape(-1, D)  # [N, D] with N=B*S\n",
    "\n",
    "        # 1) Shared path: Process the input through all shared experts and sum their outputs.\n",
    "        shared_out = torch.zeros_like(x)\n",
    "        for exp in self.shared:\n",
    "            shared_out += exp(x)\n",
    "        # (Optional) Scale the shared expert output by the number of shared experts.\n",
    "        # This can help in balancing the contribution of shared vs. routed experts.\n",
    "        # shared_out = shared_out / max(1, self.n_shared)\n",
    "\n",
    "        # 2) Router logits: Calculate the affinity of each token to each routed expert.\n",
    "        # Use autocasting to FP16 if fp16_router is True and the device is CUDA.\n",
    "        use_autocast = self.fp16_router and x.is_cuda\n",
    "        device_type = \"cuda\" if x.is_cuda else x.device.type\n",
    "        with torch.autocast(device_type=device_type, enabled=use_autocast):\n",
    "            # Calculate logits by taking the dot product of the flattened input with the expert centroids.\n",
    "            logits = F.linear(x_flat, self.centroids)  # [N, E]\n",
    "            # Add the router bias to the logits. Ensure bias matches the logits' dtype.\n",
    "            logits = logits + self.bias.to(logits.dtype)\n",
    "\n",
    "        # Select the top_k experts with the highest logits for each token.\n",
    "        topk_logits, topk_idx = torch.topk(logits, self.top_k, dim=-1)        # [N, k]\n",
    "        # Apply softmax to the top_k logits to get gating weights.\n",
    "        # Ensure the gate weights have the same dtype as the input for subsequent calculations.\n",
    "        gate = F.softmax(topk_logits, dim=-1, dtype=x.dtype)                   # [N, k]\n",
    "\n",
    "        # 3) Dispatch per expert: Route tokens to their selected experts and combine outputs.\n",
    "        routed_out = torch.zeros_like(x_flat)                                   # [N, D]\n",
    "        # Iterate through each routed expert.\n",
    "        for i in range(self.n_routed):\n",
    "            # Create a mask to identify which tokens selected the current expert (expert i).\n",
    "            mask = (topk_idx == i)\n",
    "            # Find the indices of the rows (tokens) and columns (which of the top-k) where expert i was selected.\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)                      # 1-D each\n",
    "            # If no tokens selected this expert, skip.\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "            # Select the input tokens that are routed to expert i.\n",
    "            exp_in = x_flat.index_select(0, row_idx)                            # [Ti, D] where Ti is the number of tokens routed to expert i\n",
    "            # Pass the selected tokens through the expert's FFN.\n",
    "            out = self.routed[i](exp_in)                                        # [Ti, D]\n",
    "            # Get the gating weights for the tokens routed to expert i.\n",
    "            w = gate[row_idx, which_k].unsqueeze(-1)                            # [Ti, 1]\n",
    "            # Scale the expert output by the gating weights and add it to the routed_out tensor\n",
    "            # at the original token positions using index_add_.\n",
    "            routed_out.index_add_(0, row_idx, out * w)\n",
    "\n",
    "        # Reshape the routed output back to the original [B, S, D] shape.\n",
    "        routed_out = routed_out.view(B, S, D)\n",
    "        # The final output is the sum of the original input, shared expert outputs, and routed expert outputs.\n",
    "        return x + shared_out + routed_out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_bias(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Updates the router bias based on expert load.\n",
    "\n",
    "        This method is typically called once per optimizer step using the\n",
    "        same batch of tokens that were passed through the forward method.\n",
    "        It uses the current router logits (including the current bias) to\n",
    "        estimate the load on each expert and adjusts the bias to encourage\n",
    "        a more balanced distribution of tokens across experts.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape [B, S, D], identical\n",
    "                              to the input used in the corresponding forward pass.\n",
    "        \"\"\"\n",
    "        # Calculate the total number of tokens.\n",
    "        N = x.shape[0] * x.shape[1]\n",
    "        # Calculate the router logits (affinity scores) for each token and expert, including the current bias.\n",
    "        logits = F.linear(x.reshape(-1, self.d_model), self.centroids) + self.bias\n",
    "        # Determine the top_k experts selected for each token based on the current logits.\n",
    "        _, idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "\n",
    "        # Count how many times each expert was selected as one of the top_k.\n",
    "        counts = torch.bincount(idx.flatten(), minlength=self.n_routed).float()\n",
    "        # Calculate the average number of times an expert should ideally be selected.\n",
    "        avg = counts.sum() / max(1, self.n_routed)\n",
    "\n",
    "        # Calculate the \"violation\" for each expert. A positive violation means\n",
    "        # the expert is under-loaded compared to the average, and its bias\n",
    "        # should be increased to make it more likely to be selected in the future.\n",
    "        # A negative violation means it's over-loaded, and its bias should be decreased.\n",
    "        # Add a small epsilon (1e-6) to the denominator to avoid division by zero.\n",
    "        violation = (avg - counts) / (avg + 1e-6)\n",
    "        # Update the bias using a smooth, bounded update based on the violation.\n",
    "        # torch.tanh() squashes the violation into the range [-1, 1], preventing\n",
    "        # excessively large bias updates. The bias_lr controls the step size.\n",
    "        self.bias.add_(self.bias_lr * torch.tanh(violation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b77cc5",
   "metadata": {},
   "source": [
    "## Expert Network Implementation\n",
    "\n",
    "### GELU Activation Function\n",
    "\n",
    "We begin with a custom implementation of the GELU (Gaussian Error Linear Unit) activation function. This approximation is slightly faster than the standard implementation:\n",
    "\n",
    "$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right)\\right)$\n",
    "\n",
    "### ExpertFFN Class\n",
    "\n",
    "The `ExpertFFN` class implements a simple 2-layer MLP (Multi-Layer Perceptron) that serves as an individual expert in our MoE architecture:\n",
    "\n",
    "1. **Architecture**: Input → Linear → GELU → Dropout → Linear → Output\n",
    "2. **Parameters**:\n",
    "   - `d_model`: Input/output dimension (model dimension)\n",
    "   - `hidden`: Hidden layer dimension (typically smaller than in standard FFNs)\n",
    "   - `dropout`: Dropout rate for regularization\n",
    "\n",
    "In DeepSeek-V3, the hidden dimension for experts is approximately 0.25 × d_model, making each expert significantly smaller than a traditional FFN. This is part of what makes MoE efficient - the experts are individually small.\n",
    "\n",
    "### DeepSeekMoE Class\n",
    "\n",
    "The `DeepSeekMoE` class implements DeepSeek's innovative approach to Mixture of Experts. It combines several key design choices:\n",
    "\n",
    "1. **Hybrid Expert Structure**:\n",
    "   - **Shared Experts**: Always active for all tokens\n",
    "   - **Routed Experts**: Selectively activated based on token content\n",
    "\n",
    "2. **Top-K Routing**:\n",
    "   - Each token activates K out of E total experts\n",
    "   - Weighted combination of expert outputs\n",
    "\n",
    "3. **Router Design**:\n",
    "   - Uses simple dot-product similarity with learned centroids\n",
    "   - Optional mixed-precision routing for efficiency\n",
    "   - Dynamic bias adjustment for load balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377dd88b",
   "metadata": {},
   "source": [
    "## Understanding the Forward Pass\n",
    "\n",
    "The forward method in `DeepSeekMoE` contains the core logic of how tokens are processed through both shared and routed experts. Let's break down the key steps:\n",
    "\n",
    "### 1. Shared Path\n",
    "\n",
    "All tokens are processed by all shared experts, and their outputs are summed. This ensures a base level of processing for every token regardless of routing decisions.\n",
    "\n",
    "### 2. Router Calculation\n",
    "\n",
    "The router uses a simple but effective approach:\n",
    "- Computes dot-product similarity between tokens and expert centroids\n",
    "- Adds the dynamic bias to adjust for load balancing\n",
    "- Optionally uses mixed precision for efficiency on GPUs\n",
    "- Selects the top-k experts for each token based on similarity scores\n",
    "- Normalizes the scores with softmax to get gate values (weights)\n",
    "\n",
    "### 3. Selective Expert Computation\n",
    "\n",
    "For each expert, the implementation:\n",
    "- Identifies which tokens selected this expert\n",
    "- Processes only those tokens (avoiding unnecessary computation)\n",
    "- Weights the outputs by the corresponding gate values\n",
    "- Accumulates the weighted outputs in the final result\n",
    "\n",
    "### 4. Residual Connection\n",
    "\n",
    "Finally, the original input is added to both the shared and routed paths, forming a residual connection that helps with gradient flow and training stability.\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "The separate `update_bias` method implements an online load balancing mechanism that adjusts the router's bias terms to encourage more balanced expert utilization over time. This is crucial for preventing the \"rich get richer\" problem where a few experts might dominate the routing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86808170",
   "metadata": {},
   "source": [
    "## Testing the DeepSeek MoE Implementation\n",
    "\n",
    "Let's test our implementation with some sample data to verify it's working correctly. We'll create a model with:\n",
    "\n",
    "- Model dimension (`d_model`): 1024\n",
    "- Number of routed experts: 16\n",
    "- Number of shared experts: 2\n",
    "- Top-K experts per token: 8\n",
    "\n",
    "Then we'll pass a batch of random token embeddings through the model and verify the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DeepSeekMoE class\n",
    "d_model = 1024\n",
    "n_routed_exp = 16\n",
    "n_shared_exp = 2\n",
    "top_k = 8\n",
    "\n",
    "model = DeepSeekMoE(d_model, n_routed_exp, n_shared_exp, top_k)\n",
    "\n",
    "# Create different random input data\n",
    "batch_size_new = 2\n",
    "seq_len_new = 64\n",
    "random_input_new = torch.randn(batch_size_new, seq_len_new, d_model)\n",
    "\n",
    "# Pass the new random input to the model's forward method\n",
    "output_new = model(random_input_new)\n",
    "\n",
    "print(\"New output shape:\", output_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa51d9",
   "metadata": {},
   "source": [
    "## Conclusion: The Impact of MoE in DeepSeek\n",
    "\n",
    "DeepSeek's implementation of Mixture of Experts represents a significant advancement in large language model architecture. By combining always-on shared experts with selectively activated routed experts, DeepSeek achieves:\n",
    "\n",
    "1. **Parameter Efficiency**: The model can have a vast number of parameters (hundreds of billions) while only activating a small fraction for any given token.\n",
    "\n",
    "2. **Computational Efficiency**: By activating only top-k experts per token, computation scales sub-linearly with the number of parameters.\n",
    "\n",
    "3. **Quality Preservation**: The shared experts ensure that all tokens receive high-quality processing, addressing a common weakness of pure sparse MoE models.\n",
    "\n",
    "4. **Load Balancing**: The dynamic bias adjustment mechanism prevents expert imbalance, ensuring all experts are utilized effectively.\n",
    "\n",
    "This architecture is a key reason why DeepSeek models can achieve state-of-the-art performance while maintaining reasonable inference costs. The hybrid approach strikes an excellent balance between the pure dense models (like GPT) and pure sparse models (like earlier MoE architectures), getting the best of both worlds.\n",
    "\n",
    "In a full model implementation, these MoE layers would replace some or all of the standard FFN layers in transformer blocks, while keeping the attention mechanisms unchanged."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
